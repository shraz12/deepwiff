{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Mixture_Density_Network.ipynb","provenance":[{"file_id":"1PAU0JdrtTFrbYBq6zL2z-36N6-St_0lm","timestamp":1607117522414},{"file_id":"1AB4ogjK3lQveiLvbqUYONuacLWlwoKFV","timestamp":1606786393510}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"fp0rwy5rGb7-"},"source":["'''\n","Mixture density neural network with 4 hidden dense layers \n","and 1 mixture density layer to predict output distribution.\n","Preprocessing code written by Prof. Horvat, network code written by Meal Swipes for Late Days.\n","Input is a (batch_size x 26) tensor representing the input wave spectrum, \n","output is a (batch size x 49) tensor representing the output floe size distribution.\n","'''\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from scipy.io import loadmat,savemat\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_probability as tfp\n","import random\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping\n","from google.colab import drive "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWTcM43_Gb8A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607675300550,"user_tz":300,"elapsed":22521,"user":{"displayName":"Shreyas Rao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFlxGUENHol0FW_X9ONfmnBYzvdCNq7uqxyi8GnA=s64","userId":"03678414745665861573"}},"outputId":"9f460876-37d8-49a8-b2ac-93678bcfb801"},"source":["# Define file locations\n","\n","usable_class = 0\n","\n","include_last = True;\n","\n","drive.mount('/content/drive', force_remount=True)\n","# locstr = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/FSTD-Ongoing-Work/Neural-Net-Waves/'\n","locstr = '/content/drive/Shared drives/DeepWIFF/'\n","\n","training_string = locstr + 'Data/training_data_errtol'\n","\n","if not include_last:\n","    training_string = training_string + '_nolast'\n","\n","training_string = training_string + '.mat'\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLcBHshGGb8A","executionInfo":{"status":"ok","timestamp":1607675314571,"user_tz":300,"elapsed":36531,"user":{"displayName":"Shreyas Rao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFlxGUENHol0FW_X9ONfmnBYzvdCNq7uqxyi8GnA=s64","userId":"03678414745665861573"}},"outputId":"fb5e3308-e50d-4fda-f6df-02b076e4e441"},"source":["'''\n","Loads training data and performs preprocessing to remove degenerate inputs.\n","There is no need to run this cell once the pickle files have been saved once.\n","'''\n","\n","training_data = loadmat(training_string)\n","misc_data = loadmat(locstr + 'Data/NN_params.mat')\n","\n","input_vec = training_data['input_vec'].T\n","output_vec = training_data['output_vec'].T\n","\n","Freq = misc_data['f']\n","dFreq = Freq * (np.sqrt(1.1) - np.sqrt(1/1.1)); \n","Redge = misc_data['redge'].T\n","Rcent = misc_data['rcent'].T\n","Lambda = misc_data['lambda']\n","\n","\n","num_frac = np.sum(output_vec,axis=1)\n","# classifier = load_model('Classifiers/KERAS-Classifier-25x25.h5')\n","# thresh = 0.11;\n","# y_pred_class = classifier.predict(input_vec)\n","# usable = y_pred_class[:,1] > thresh\n","\n","usable = num_frac > 0\n","print('Out of ' + str(len(num_frac)) + ', ' + str(sum(usable)) + ' can be used')\n","\n","ice_thick = input_vec[:,-1]\n","wave_spec = input_vec[:,:25]\n","\n","# wave_spec[wave_spec < 10**-8] = 0\n","wave_energy = np.sum(wave_spec*np.expand_dims(dFreq,axis=1).T,axis=1)\n","peak_loc = np.argmax(wave_spec,axis=1)\n","peak_freq = Freq[peak_loc]\n","\n","NN_input = input_vec[usable]\n","NN_output = output_vec[usable]\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Out of 1830900, 404503 can be used\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z8I8k03NGb8A"},"source":["'''\n","Loads training and testing data from pickle file.\n","'''\n","import pickle\n","\n","try:\n","\n","    X_train = pickle.load( open(locstr +  \"Data/\" + \"X_train.p\", \"rb\" ) )\n","    X_test = pickle.load( open( locstr + \"Data/\" + \"X_test.p\", \"rb\" ) )\n","    y_train = pickle.load( open(locstr + \"Data/\" + \"y_train.p\",\"rb\"))\n","    y_test = pickle.load( open(locstr + \"Data/\" + \"y_test.p\",\"rb\"))\n","    \n","except:\n","        \n","        print('exception in loading segmented data')\n","        X_train, X_test, y_train, y_test = train_test_split(NN_input,NN_output,test_size=0.3)\n","        \n","        pickle.dump(X_train, open( locstr + \"Data/\" + \"X_train.p\", \"wb\" ) )\n","        pickle.dump(X_test, open( locstr + \"Data/\" + \"X_test.p\", \"wb\" ) )\n","        pickle.dump(y_train, open( locstr + \"Data/\" + \"y_train.p\", \"wb\" ) )\n","        pickle.dump(y_test, open( locstr + \"Data/\" + \"y_test.p\", \"wb\" ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qDV69_KY5TI","executionInfo":{"status":"ok","timestamp":1607675319415,"user_tz":300,"elapsed":41360,"user":{"displayName":"Shreyas Rao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFlxGUENHol0FW_X9ONfmnBYzvdCNq7uqxyi8GnA=s64","userId":"03678414745665861573"}},"outputId":"36b68183-2fb1-422d-b291-56335e80a4fe"},"source":["tf.print(\"X_train mean: \", tf.math.reduce_mean(X_train))\n","tf.print(\"X_train stdev: \", tf.math.reduce_std(X_train))\n","tf.print(\"y_train mean: \", tf.math.reduce_mean(y_train))\n","tf.print(\"y_train stdev: \", tf.math.reduce_std(y_train))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train mean:  1.1930075366304653\n","X_train stdev:  4.3379647876911962\n","y_train mean:  0.020408163265306121\n","y_train stdev:  0.066589406428670186\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HX68qdm5u_VP"},"source":["# Normalize input vectors featurewise (or don't)\n","def featureNorm(data):\n","    mean = tf.expand_dims(tf.math.reduce_mean(data, axis = 1), axis = 1)\n","    std = tf.expand_dims(tf.math.reduce_std(data, axis = 1), axis = 1)\n","    return tf.math.divide(tf.math.subtract(data, mean), std)\n","\n","#X_train = featureNorm(X_train)\n","#X_test = featureNorm(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ty5YU2iGWvQ"},"source":["#Mixture density layer definition.\n","tf.keras.backend.set_floatx('float64')\n","class MixtureDensity(tf.keras.layers.Layer):\n","    # This layer generates a mixture distribution learned from a batch of 1d feature vectors\n","    def __init__(self, num_components, input_dim):\n","        \"\"\"\n","\t\tIntializer for a MixtureDensity layer. Creates layer and initializes the trainable variables as keras Dense layers,\n","        with respective activation functions.\n","\n","\t\t:param num_components:  integer tensor, denoting the number of learned Gaussian distributions mixed together\n","\t\t:param input_dim:  integer tensor, the 1D input dimension of the values input into the MixtureDensity layer\n","\t\t\"\"\"\n","\n","        # data_mean and data_std are set the mean value for the mu and sigma networks respectively\n","        super(MixtureDensity, self).__init__()\n","        \n","        self.regularizer = {} #{'kernel_regularizer': keras.regularizers.l2(0.001), 'bias_regularizer': keras.regularizers.l2(0.001)}\n","        self.num_components = num_components\n","        \n","        # Dense layer to generate mixing parameters (softmax ensures that alpha values sum to 1)\n","        self.alpha = keras.layers.Dense(num_components, activation = 'softmax', **self.regularizer)\n","        \n","        #  Dense layer to generate mean parameters\n","        self.mu = keras.layers.Dense(num_components, **self.regularizer)\n","        \n","        # Dense layer to generate standard deviation parameters (reluing to debug invalid parameter outputs)\n","        self.sigma = keras.layers.Dense(num_components, activation= 'softplus', **self.regularizer)\n","\n","    def call(self, inputs):\n","         \"\"\"\n","\t\tCall function for MixtureDensity layer. Passes inputs through the MixtureDensity layer and outputs a mixture of Gaussian\n","        distributions based on the learned parameters alpha, mu, and sigma. Mu and sigma represent the learned means and standard\n","        deviations, repectively, while alpha represents the mixture parameter, which determines how much of each of the num_components\n","        distributions are mixed in to the final output.\n","\n","\t\t:param inputs: 2D tensor, representing a batch of inputs to be called upon and passed throught the MixtureDensity layer\n","\n","        :return distribution: a tensor of probability distributions\n","\t\t\"\"\"\n","\n","        #(Small offset values should prevent any values <=0 after relu)\n","        alpha = tf.math.add(self.alpha(inputs), 1e-7)\n","        mu = self.mu(inputs)\n","        sigma = tf.math.add(self.sigma(inputs), 1e-7)\n","        \n","        # Wrap mixture parameters for Mixture constructor\n","        cat = tfp.distributions.Categorical(probs = alpha, allow_nan_stats= False)\n","        \n","        # Generate a list of batched component distributions to pass to the Mixture constructor\n","        dists = [tfp.distributions.Normal(loc = mu[:, comp], scale = sigma[:, comp], allow_nan_stats=False) for comp in range(self.num_components)]\n","\n","        return tfp.distributions.Mixture(cat, dists, allow_nan_stats=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ppn62q_IQ8y"},"source":[" #Mixture density model definition.\n"," class DensityModel(tf.keras.Model):\n","    \n","    def __init__(self, num_bins):\n","        \"\"\"\n","\t\tIntializer for a Mixture Density model. Intializes all the trainable variables and hyperparameters to be used \n","        throughout the model.\n","\n","\t\t:param num_bins:  integer tensor, denoting the 1D dimension of the output. Will be a histogram with num_bins bins, and a \n","                            probability distribution.\n","\t\t\"\"\"\n","\n","        super(DensityModel, self).__init__()\n","        \n","        # Training hyperparamaters\n","        self.batch_size = 256\n","        self.learning_rate = .001\n","        self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.learning_rate)\n","\n","        # Dense layer hyperparameters\n","        self.hidden_architecture = [100, 100, 100, 100]\n","        self.activation = tf.keras.layers.LeakyReLU(alpha=0.2)\n","        self.regularizer = {} #{'kernel_regularizer': keras.regularizers.l2(0.001), 'bias_regularizer': keras.regularizers.l2(0.001)}\n","\n","        # Discretization parameters\n","        self.num_components = 12\n","        self.num_bins = num_bins\n","        self.lower_bin_val = 0\n","        self.upper_bin_val = 1\n","\n","        # Revised for easier experimenting\n","        self.hidden_layers = [keras.layers.Dense(size, activation = self.activation, **self.regularizer) for size in self.hidden_architecture]\n","        self.mixture_density = MixtureDensity(self.num_components,self.hidden_architecture[-1])\n","        \n","        # One time bin index calculation for discretize\n","        self.bin_inds = tf.convert_to_tensor(np.linspace(start = self.lower_bin_val, stop = self.upper_bin_val, num = self.num_bins + 1), dtype = tf.float64)\n","\n","    def discretize(self, distributions, include_tails = False):\n","        \"\"\"\n","\t\tNovel optimized discretize function for the mixture density model. Turns continuous distributions into\n","        discrete probability distributions by using the CDF to calculate what proportion of values falls within\n","        a given histogram been. Similar process to integer rounding.\n","\n","\t\t:param distributions:  tensor of tfp distributions, all the distributions to discretize by the function\n","\t\t:param include_tails:  boolean, determines whether or not to include all the tail values within the discretization\n","\n","        :return discrete_dist: a tensor of discretized distributions\n","\t\t\"\"\"\n","\n","        cdf_per_bin = tf.vectorized_map(fn = lambda bin: distributions.cdf(bin), elems = self.bin_inds)\n","        \n","        if include_tails:\n","            # This argument modifies the boundary bins to extend across the computed distributions' tails if true\n","            left_tail = tf.expand_dims(cdf_per_bin[0, :], axis = 0)\n","            right_tail = tf.expand_dims(1 - cdf_per_bin[-1, :], axis = 0)\n","            histogram_interior = tf.math.subtract(cdf_per_bin[2:-1, :], cdf_per_bin[1:-2, :])\n","            discrete_dist = tf.concat([left_tail, histogram_interior, right_tail], axis = 0)\n","        \n","        else:\n","            discrete_dist = tf.math.subtract(cdf_per_bin[1:, :], cdf_per_bin[:-1, :])\n","        \n","        return tf.transpose(discrete_dist)\n","\n","    def discretize_legacy(self, distributions):\n","        \"\"\"\n","\t\tOutdated version of the discretize function described above. Discretizes a continuous probability distribution,\n","        but much more slowly than discretize(). It is also possibly more numerically stable and less prone to causing\n","        overflow errors.\n","\n","\t\t:param distributions:  tensor of tfp distributions, all the distributions to discretize by the function\n","        \n","        :return discrete_dist: a tensor of discretized distributions\n","\t\t\"\"\"\n","        \n","        bins = tf.range(self.num_bins)\n","        binner = lambda ind:distributions.cdf(self.bin_inds[ind+1]) - distributions.cdf(self.bin_inds[ind])\n","        discrete_dist = tf.map_fn(fn = binner, elems = bins, fn_output_signature = tf.float32)\n","        \n","        return tf.transpose(discrete_dist)\n","\n","    def call(self, input, training = False, ret_discrete = True):\n","        \"\"\"\n","\t\tCall function for MixtureDensity model. Passes input through MixtureDensity network and returns tensor of probability\n","        distributions. If ret_discrete is true, the discretizes model into self.num_bins bins, otherwise returns continuous \n","        distributions. \n","\n","\t\t:param input:  tensor, tensor representing multiple 1d vectors to be passed through the call function \n","        :param training, boolean, indicates whether model is training or testing\n","        :param ret_discretem boolean, indicates whether or not to discretize the return data\n","\n","        :return distribution: a tensor of discrete/continuous probability distributions\n","\t\t\"\"\"\n","        layer_out = input\n","        for layer in self.hidden_layers:\n","            layer_out = layer(layer_out)\n","            #layer_out = tf.nn.dropout(layer_out, 0.2)\n","\n","        distribution = self.mixture_density(layer_out)\n","        if ret_discrete:\n","            return self.discretize(distribution)\n","        else:\n","            return distribution\n","\n","    def loss(self, labels, discretized_distributions, loss_type = tf.keras.losses.MSE):\n","        return -loss_type(labels,discretized_distributions)\n","        \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"DFlJPrAdGb8A","colab":{"base_uri":"https://localhost:8080/","height":491},"executionInfo":{"status":"error","timestamp":1607680758802,"user_tz":300,"elapsed":340570,"user":{"displayName":"Shreyas Rao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFlxGUENHol0FW_X9ONfmnBYzvdCNq7uqxyi8GnA=s64","userId":"03678414745665861573"}},"outputId":"fc7945d3-1a21-4053-aaf2-84dcf07a3340"},"source":["#Creating the Mixture Density model\n","MDN = DensityModel(49)\n","\n","\"\"\"\n","Compiles all the data for training and testing the Mixture Desnity model. Creates a list of different loss functions to keep track of.\n","\"\"\"\n","losses = [\"mse\", \"kld\", \"mae\"]\n","MDN.compile(optimizer = MDN.optimizer, loss = losses[0], metrics = [\"kullback_leibler_divergence\",\"mean_squared_error\", \"mean_absolute_error\"])\n","\n","# Output name\n","outstr = locstr + 'Network Weights/MDN-' + str(MDN.num_components)\n","\n","if not include_last:\n","    outstr = outstr + '-nolast'        \n","\n","#Adds early stopping, if so desired\n","es = EarlyStopping(monitor='val_loss', verbose=1, patience=10);\n","\n","# Reduce learning rate on plateau\n","lradj = keras.callbacks.ReduceLROnPlateau()\n","\n","#Initializing necessary TensorBoard metrics for better metrics\n","tb_log = tf.keras.callbacks.TensorBoard(\n","    log_dir = locstr + \"logs\",\n","    histogram_freq = 5,\n","    write_graph = True,\n","    update_freq= 50,\n","    profile_batch=2,\n","    embeddings_freq=1)\n","\n","\"\"\"\n","Trains and tests the mixture density network. Validation_data = data to be tested. Automatically trains and tests for 100 epochs\n","\"\"\"\n","history = MDN.fit(X_train,y_train, validation_data = (X_test,y_test), verbose = 1, batch_size = MDN.batch_size, epochs=100, callbacks = [lradj])\n","\n","#Saving the weights of the mixture density model\n","MDN.save_weights(outstr+\".h5\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","1107/1107 [==============================] - 113s 102ms/step - loss: 0.0029 - kullback_leibler_divergence: 2.6816 - mean_squared_error: 0.0029 - mean_absolute_error: 0.0187 - val_loss: 0.0020 - val_kullback_leibler_divergence: 1.9155 - val_mean_squared_error: 0.0020 - val_mean_absolute_error: 0.0145\n","Epoch 2/100\n","1107/1107 [==============================] - 101s 91ms/step - loss: 0.0013 - kullback_leibler_divergence: 0.9304 - mean_squared_error: 0.0013 - mean_absolute_error: 0.0120 - val_loss: 0.0013 - val_kullback_leibler_divergence: 0.9053 - val_mean_squared_error: 0.0013 - val_mean_absolute_error: 0.0118\n","Epoch 3/100\n","1106/1107 [============================>.] - ETA: 0s - loss: 0.0012 - kullback_leibler_divergence: 0.7289 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0113"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-74c5e2c97456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mTrains\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtests\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmixture\u001b[0m \u001b[0mdensity\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mValidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mtested\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mAutomatically\u001b[0m \u001b[0mtrains\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtests\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlradj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#Saving the weights of the mixture density model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"g-Cc2263dBNC"},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrgmhQeSGb8A"},"source":["\"\"\"\n","Saves all the training and testing data collected above, and writes data to a file.\n","\"\"\"\n","\n","print(history.history)\n","pickle.dump(history.history, open( locstr + \"Data/\" + \"MDN-100x100x100x100x10-MSE-FAKE.p\", \"wb\" ))\n","plt.plot(log(history.history['loss']))\n","plt.plot(log(history.history['val_loss']))\n","np.min(history.history['val_loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8sp8Us6mGb8A"},"source":["#For assessing network performance, load saved model weights\n","model_name = ''\n","\"\"\"\n","Recompile model to load weights. Keras requires that predict() be called to initialize the model layers\n","\"\"\"\n","MDN = DensityModel(49)\n","MDN.compile(optimizer = MDN.optimizer, loss = losses[0], metrics = [\"kullback_leibler_divergence\",\"mean_squared_error\", \"mean_absolute_error\"])\n","MDN.predict(X_test[0])\n","\n","MDN = load_weights(locstr + \"Network Weights/\" + model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9BBzcMWq1EfC"},"source":["eval = MDN.evaluate(X_test, y_test, return_dict = True)\n","for name in MDN.metrics_names:\n","    print(\"%s: %f\" % (name, eval[name]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k82sRNx6Gb8A"},"source":["# q = random.randint(0,y_test.shape[0])\n","# plt.semilogx(Rcent,y_test[q,:])\n","\n","# test_out = MDN.predict(np.expand_dims(X_test[q,:],axis=1).T).T\n","\n","# plt.semilogx(Rcent,test_out)\n","\n","# print('MAE value here is ' + str(sum(abs(np.subtract(np.expand_dims(y_test[q,:],axis=1),test_out)))))\n","\n","test_out = MDN.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RvWTFn9KGb8B"},"source":["#Prints out certain error metrics useful to the model\n","errmat = np.sum(np.abs((test_out - y_test)),axis=1)\n","errmat2 = np.mean((test_out - y_test)**2,axis=1)\n","testE = np.sum(X_test[:,:25]*dFreq.T,axis=1)\n","testH = X_test[:,-1]\n","\n","print(errmat.shape)\n","print(testE.shape)\n","print(testH.shape)\n","\n","print('SAE is: ' + str(np.nanmean(errmat)))\n","print(\">10% SAE is: \" + str(np.sum(errmat > 0.1)/len(X_test)))\n","print('MSE is: ' + str(np.nanmean(errmat2)))\n"],"execution_count":null,"outputs":[]}]}